# Python爬虫项目
'''
浏览器中输入url网址，浏览器根据网址向指定的服务器发送请求
服务器响应请求，返回请求内容
浏览器渲染，展示给客户
浏览器不管代码哪来的，只负责渲染，所以有了爬虫

爬虫爬取数据原理
第一步：构造一个请求
第二步：发送请求，获取服务端的响应
第三步：从响应内容里提取我们所需要的特定数据

爬虫的应用场景与作用
在信息时代，面对海量数据
我们需要一种更加便捷高效的手段
来帮助我们进行数据的获取与分析

B/S结构的软件。也就是我们网站
从而有了HTTP服务器，类似于路由的作用
根据URL的请求进行响应
一般情况：有三种响应内容
1，最常见，响应一个html文件的内容
2，也很常见，响应一个数据
3, 响应静态资源（图片，媒体）

项目主旨：在小说网站中，下载一本小说
要用到的工具：import urllib2
urllib2 Python标准库给开发人源提供的一系列针对url的操作方法
urllib Python标准库给开发人员提供一系列的针对url的操作方法
Python2中，两者并不是相互替代的关系，互补的关系
urllib2可以让我们自己设置请求头
urllib不可以

import re
python标准库 给开发人员提供的一系列针对字符串的操作方法
re.
'''

# HTTP简介

HTTP是超文本传输协议，是用于万维网服务器传输超文本到本地浏览器的传输协议

是一个基于TCP/IP通信协议来传输数据（HTML,文件，图片等）

是一个属于应用层的面向对象的协议，由于简单，快速的方式，适用于分布式超媒体信息系统
于1990年被提出，经过不断发展与完善，目前在WWW中使用的是HTTP/1.0的第六个版本，HTTP/1.1的规范化正在进行中
并且HTTP-NG（HTTP的下一代）的建议也已经提出来了 

HTTP协议工作在客户端-服务器架构位上，浏览器作为HTTP客户端通过URL向HTTP服务端即Web服务器发送所有请求，
Web服务器根据收到的请求，向客户端发送响应信息

# 主要特点
1、简单快速:客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST.每种方法规定了客户与服务器联系的类型不同。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。

2、灵活:HTTP允许传输任意类型的数据对象。正在传输的类型由Content- Type加以标记。

3.无连接:无连接的含义是限制每次连接只处理一个请求。 服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。

4.无状态: HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。

5. 支持B/S及CS模式。

#  HTTP之Url

HTTP使用统一统资源标识符(Uniform Resource lentiers URI)来传输数据和建立连接。URL是种特殊类型的URI,包含了用于查找某个资源的足够的信息

URL,全称是UnitormResourcel ocator,中文叫统一资源定位符，是互联网上用来标识某处资源的地址。以下面这个URL为例，介绍下普通URL的各部分组成:

http://www. aspxfans com:8080/news/index.asp?boardiD 5&ID-24618&page= 1#name
从上面的URL可以看出，一个完整的URL包括以下几部分: 

1.协议部分: 该URL的协议部分为"http:”，这代表网页使用的是HTTP协议。在Internet中可以使用多种协议，如HTTP, FTP等等本例中使用的是HTTP协议。在"HTTP”的后面"//"分隔符
2.域名部分:该URL的域名部分为"www aspxtans com.一个URL中，也可以使用IP地址作为域名使用
3.端口部分：跟着域名后面的是端口，域名和端口之间使用““作为分隔符。端口不是一个URL必须的部分，如果省略端口部分，将采用默认端口
4.虚拟目录部分:从域名后的第一个"/"开始到最后一个”为止,是虚拟目录的部分。虚拟目录也不是一个URL必须的部分，本例中的虚拟目录是"new/s"
5.文件部分，最后一个"/"到"#",是文件部分，如果没有"#,/",那么从城名后的最后个"/"开始到结束， 都是文件名部分。本例中的文件名是"index. asp".文件名部分也不是一个URL必须的部分， 如果省略该部分，则使用默认的文件名
6.锚部分:从“#"开始到最后，都是锚部分。本例中的锚部分是"name".锚部分也不是一个URL 必须的部分
7.参数部分:从"? "开始到“#”为止之间的部分为参数部分，又称搜索部分、查询部分。本例中的参数部分为"boardlD=5&lD=24618&page=-1".参数可以允许有多个参数，参数与参数之间用“&”作为分隔符。(原文: htp:p:/li/ csdn.net/ergouge/aricieldelails/8185219)


# HTTP请求之Request
客户端发送一个HTTP到服务器的请求，包括一下格式：请求行（请求方法 URL 协议版本），请求头部（键值对），空行，请求数据四部分
Http请求消息结构.png请求行以一个方法符号开头，以空格分开，后面跟着请求的URI和协议的版本。GET请求例子，使用Charles抓取的request:

GET 62152800001016000338Jj0 HTTP/1.1
Host img.mukewang.com
User-Agent M2ial5.0 (Wndows NT 10.0 WOW64) AplWebee 537 36 KHM ike GeckChrome/51.0.2704.106 Satar/537 36

Accept image/webp.image/,/";q=0.8
Referer htp:/www.imooc com/
Accept-Encoding gzip, detlate, sdch
Accept-L anguage zh-CN,zh:q=0.8
"
第一部分:请求行，用来说明请求类型，要访问的资源以及所使用的TTP版本，
GET说明请求类型为ET:1525258000016106000338. ipg]为要访问的资源，该行的最后-部分说明使用的是HTTP1.1版本。

第二部分:请求头部，紧接着请求行(即第一行)之后的部分，用来说明服务器要使用的附加信息
从第二行起为请求头部，HOST将指出请求的目的地.User-Agent,服务器端和客户端脚本都能访问它，它是浏览器类型检测逻辑的重要基础该信息由你的浏览器来定义，并且在每个请求中自动发送等等

第三部分:空行，请求头部后面的空行是必须的
即使第四部分的请求数据为空，也必须有空行。
第四部分:请求数据也叫主体，可以添加任意的其他数据。
这个例子的请求数据为空。

POST请求例子，使用Charles抓取的request:
POST /HTTP1.1
SHOT ON REDMI NOTE 5MI DUAL CAMERA e



# HTTP响应
http响应消息格式:jpg例子

HTTP/1.1 200 OK
Date: Fri, 22 May 2009 06:07:21 GMT
Content-Type: text/htm; charset-UTF-
html响应

第一部分:状态行，由HTTP协议版本号， 状态码，状态消息 三部分组成。
第一行为状态行，(HTTP/1.1) 表明HTTP版本为1.1版本，状态码为200,状态消息为(ok)
第二行：消息报头，用来说明客户端要使用的一些附加信息
第二行与第三行作为消息报头，Date生成响应的时间，，Content—Type指定MIME类型

# 总结
1，GET提交，请求的数据会附在url之后（就是把数据放在HTTP协议头中），以？分隔url和传输数据，多个参数用&链接。eg：
login.action?name-hyddd&password-idontknow&verify=%E4%BD%A0 %E5%A5%BD
如果数据是英文字母/数字，原样发送，如果是空格，转换为+,如果是中文/其他字符，则直接把字符串用BASE64加密，得出如: %E4%BD%A0%E5%A5%BD, 其中%XX中的XX为该符号以16进制表示的ASCII。

POST提交:把提交的数据放置在是HTTP包的包体中。上文示例中红色字体标明的就是实际的传输数据
因此，GET提交的数据会在地址栏中显示出来，而POST提交，地址栏不会改变
2、传输数据的大小:首先声明: HTTP协议没有对传输的数据大小进行限制，HTTP协议规范也没有对URL长度进行限制。

而在实际开发中存在的限制主要有
GET：特定浏览器和服务器对URL长度有限制，例如IE对URL长度的限制是2083字节(2K+35.对于其他没器，Nelscape. FireFox等， 理论上没有长度限制，其限制取决于操作系统的支持。
)
4、Http get.post.soap协议都是在http上运行的

(1) get:请求参数是作为一个key-value对的序列(查询字符串)附加到URL上的查询字符串的长度受到web浏览器和web服务器的限制(如Ie最多支持2048个字符) ,不适合传输大型数据集同时，它很不安全

(2) post:请求参数是在http标题的一个不同部分(名为entity body)传输的，这部分用来传输表单信息，因此必须将Content-type设置为application/x-Www-form urlencoded. post设计用来支持web窗体上的用户字段，其参数也是作为keyvalue对传输。但是: 它不支持复杂数据类型，因为post没有定义传输数据结构
(3) get方式提交数据，会带来安全问题，比如登陆页面，通过get提交数据时，用户名与密码将出现子url上，如果页面可以被缓存或其他人可以访问本主机，将可能从历史记录获取该用户的账号与密码


# Urllib和Urlib2库
urllib和urllib2库，urllib提供了 一系列用于操作URL的功能，可以让我们利用程序去执行各种HTTP请求。如果要模拟浏览器完成特定功能，需要把请求伪装成浏览器。伪装的方法是先监控浏览器发出的请求，再根据浏览器的请求头来伪装，User-Agent头就是 用来标识浏览器的。

# 区别
一、 在python中, urllib,urllib22不可相互替代的。整体来说，urllib2是urllib的增强， 但是urllib有urllib2中所没有的函数。

urllib2可以用urllib2.openurl中设置Request参数，来修改Header头。如果你访问一个网站，想更改User Agent (可以伪装你的浏览器)，你就要用urllib2.

urllib支持设置编码的函数，urllib.urlencode, 在模拟登陆的时候，经常要post编码之后的参数，所以想不使用第三方库完成模拟登录，你就需要使用urllib
ur1lib一般和ur1lib2一起搭配使用

二、urilib 和urllib2都是接受URL请求的相关模块，但是提供了不同的功能。两个最显著的不同如下:

1.urllib提供urlencode方法用来GET查询字符串的产生，而url1ib2没有。 这是为何urllib常和urllib2一起使用的原因。
2.ur11ib2可以接受1个Request类的实例来 设置URL请求的headers, ur1lib仅可以接受URL。这意味着，你不可以伪装你的User Agent字符串等 (伪装浏览器) 。
3.urllib2模块比较优势的地方是urllibur1lib2.urlopen可以接受Request对象作为参数,从而可以控制HTTP Request的header部分。
4.ullib2模块没有加入urllib.urletrieve函数以及urllib.quote等系列quote和unquote功能，因此有时也需要urllib的辅助

# BeautifulSoup库（爬虫库很多）
简介

简单来说，Beautiul Soup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下:
Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为Utt-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明下原始编码方式就可以了。Beautiul Soup已成为和xml、html6lib一样出色的python解释器， 为用户灵活地提供不同的解析策略或强劲的速度。

